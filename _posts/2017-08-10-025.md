---
layout: post
title: Theano + PyMC3 で Variational Auto Encoder
comments: true
tags: MachineLearning
---

[全体のコードと実行結果 - pymc_vae.ipynb](https://github.com/ShigekiKarita/t712/blob/master/example/pymc_vae.ipynb)

前回は少しだけ、VAEの話をしましたが、PyMC3というライブラリを使って遊んでみました。思いっきり自作のtheano系ライブラリを使っていますが、本家の例では`keras`を使っており、モデルの`theano.shared`変数を全部集めて`pymc3.fit`に渡す方法があれば、どんな`theano`系ライブラリでも活用できると思います。


備忘録として、簡単にPyMC3の解説をすると、

```python
# building model
n_latent = 2
n_batch = 64

vae = ConvVAE(n_latent)  # theano で作ったNN 
xs = tt.tensor4("xs")
xs.tag.test_value = numpy.zeros((n_batch, 1, 28, 28)).astype('float32')

with pm.Model() as model:
    zs = pm.Normal("zs", mu=0, sd=1, shape=(n_batch, n_latent),
                   dtype=theano.config.floatX, total_size=len(data))
    xs_ = pm.Normal("xs_", mu=vae.decode(zs), sd=0.1, observed=xs,
                    dtype=theano.config.floatX, total_size=len(data))
```

`data`はMNISTの学習セット全てが入った多次元配列(データ数, 1, 28, 28)です。あとは[前回のVAE]({{ site.baseurl }}/2017/08/02/024.html)にでてくる$x, z$をそのまま`xs`, `zs`としています。`pm.Model()`の中で有向グラフのような、確率変数がどういう分布(今回は$\text{xs} \sim \text{Normal}(\mu, \text{sd})$)から生成されているのかを記述します。注意点としては観測変数には`xs.tag.test_value`として適当な大きさの入力を代入しておかないと、PyMC3内部のアサーションに引っかかってコンパイルできません。


```python
# fitting model
mean, stddev = vae.encode(xs)
local_RVs = OrderedDict({zs: (mean, stddev)})  # encoded stochastic variable q(z|x)
xs_minibatch = pm.Minibatch(data, n_batch)

with model:
    approx = pm.fit(10000, local_rv=local_RVs,
                    obj_optimizer=pm.adam(learning_rate=1e-4),
                    more_obj_params=list(vae.get_params()),
                    more_replacements={xs: xs_minibatch})
```

ここで RV とは Random Variable (確率変数) のことだそうです。scikit-learnなどによくある `fit` 関数と違ってユーザが設定する損失などはなく、`pm.fit()`は変分ベイズ法なので`observed=xs`とした実際の観測データ`xs`に対する[前回導出した周辺尤度の下限]({{ site.baseurl }}/2017/08/02/024.html)(i.e., ELBO, 変分下限)を最大化するように`pm.floatX`で設定したパラメタや`theano.shared`変数を更新します。今回はニューラルネットを含んでいるので単純な勾配法(+Adamによる更新則)で最適化していますが、いくつかの分布ではもっと良い手法も使えるようです。

+ [3.3 Variational inference](http://pymc-devs.github.io/pymc3/notebooks/api_quickstart.html#3.3-Variational-inference)
+ [Variational api quickstart](http://pymc-devs.github.io/pymc3/notebooks/variational_api_quickstart.html)

ただこの感じだと、前回導出したような事前分布のKLダイバージェンスを分割できているのかとか、そもそもサンプリングの回数とか指定できるのか(たぶん `pm.sample()` を併用するんじゃないでしょうか？)。まだわかりません...。

次回に続くといいですね...(?)
