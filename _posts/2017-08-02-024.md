---
layout: post
title: 周辺尤度下界(ELBO)の導出
comments: true
tags: MachineLearning
---

周辺尤度(Marginal log-likelihood, Evidence)はパラメタの良さを測るときに使われる重要な尤度です(経験ベイズや、変分ベイズなど)。いまいち、導出の道筋がよくわかっていなかったので、メモしています。

独立同分布(i.i.d.)なデータセット $x^{(1)}, \dots, x^{(I)}$ に対してパラメタ$\theta$を持つ生成モデル$p\_{\theta}(x)$の周辺尤度は$\log p\_{\theta} (x^{(1)}, \dots, x^{(I)}) = \sum\_{i} \log p\_{\theta}(x^{(i)}) $と表すことができます。任意のパラメタ$\phi$を持つ確率分布$ \int\_z q\_\phi(z) dz = 1 $ に対して各因子は次のようになります。

$$
\begin{align}
  \log p\_{\theta}(x^{(i)}) &= \log p\_{\theta}(x^{(i)}) \int\_{z} q(z) dz \\\
  &= \mathrm{KL}(q\_\phi(z) || p\_\theta(z|x^{(i)}) ) + \int\_z q\_\phi(z) \log \frac{p\_\theta(z|x^{(i)})}{q\_\phi(z)} dz  + \int\_{z} q(z)  \log p(x^{(i)}) dz \\\
  &= \mathrm{KL}(q\_\phi(z) || p\_\theta(z|x^{(i)}) ) + \mathbf{E}\_{q\_\phi(z)} \left[ -\log q\_\phi(z) + \log p\_\theta(x^{(i)}, z) \right] \\\
\end{align}
$$

いきなりでてきた確率変数$z$は潜在変数と呼ばれ、観測できない$z$(文字認識だと認識したいテキストとか、とくに意味のない特徴量など)によって観測データ$x$(文字認識だとテキストの写ってる画像とか)が生成されるという説明が多い気がします。今回は連続値として記載していますが離散値でも$\sum\_z$に記法を変えれば全く同じ議論ができます。

同じくいきなり出てきた確率分布間の非類似度であるKLダイバージェンス $\mathrm{KL}(q || p) =  \int\_z q(z) \log \frac{p(z)}{q(z)} dz$ は解析的に計算できる分布は限られています。そこで、Gibbsの不等式(Jensen の不等式に由来)とよばれるKLダイバージェンスが非負である性質によって得られる、期待値のみで表した周辺尤度下界(ELBO)

$$
\begin{align}
  \log p\_{\theta}(x^{(i)}) 
  &= \mathrm{KL}(q\_\phi(z) || p\_{\theta}(z | x^{(i)})) + \mathbf{E}\_{q\_\phi(z)} \left[ -\log q\_\phi(z) + \log p\_\theta(x^{(i)}, z) \right] \\\
  &\geq \mathbf{E}\_{q\_\phi(z)} \left[ -\log q\_\phi(z) + \log p\_\theta(x^{(i)}, z) \right]
\end{align}
$$

を用いて、期待値の中身の分布からランダムにサンプリングした平均値による近似計算(Monte-Carloサンプリングと呼ばれる)するといったテクニックがよく使われます(任意の分布が計算できるMCサンプリングを使うために、期待値だけの式にしたかったからKLダイバージェンスの項を取り出したとも言える)。


$$
\begin{align}
  \log p\_{\theta}(x^{(i)}) &\geq \mathbf{E}\_{q\_\phi(z)} \left[ -\log q\_\phi(z) + \log p\_\theta(x^{(i)}, z) \right] \\\
  &\approx \frac{1}{J} \sum\_{j=1}^{J}  \left\\{ -\log q\_\phi(z^{(j)}) + \log p\_\theta(x^{(i)}, z^{(j)}) \right\\}, \, z^{(j)} \sim  q\_\phi(z)
\end{align}
$$


## 最近の技術

実際のところ、このままだとサンプリングによる近似の粗さ(分散の大きさ)が問題になります。近年、Variational autoencoder (VAE) などの手法では事前分布の$q\_\phi(z), p\_\theta(z)$をKLダイバージェンスが解析的に求まる分布(例. 正規分布, 多項分布など)に限定して、解析的に求まる項を導入することで対処します。

$$
\begin{align}
  \mathbf{E}\_{q\_\phi(z)} \left[ -\log q\_\phi(z) + \log p\_\theta(x^{(i)}, z) \right] 
  &= \mathbf{E}\_{q\_\phi(z)} \left[ -\log \frac{q\_\phi(z)}{p\_\theta(z)} + \log \frac{p\_\theta(x^{(i)}, z)}{p\_\theta(z)} \right] \\\
  &= - \text{KL}(q\_\phi(z) || p\_\theta(z) ) +  \mathbf{E}\_{q\_\phi(z)} \left[ \log p\_\theta(x^{(i)}| z) \right] 
\end{align}
$$

事前分布$p\_\theta(z)$に単純な分布を仮定すれば、任意の分布$q\_\phi(z)$のKLダイバージェンスが近づく(=単純になる)ような制約としても使えるので、モデルの複雑さを抑える正則化的な効果もあると言われています。第二項はサンプリングは必要になりますが、分散は小さいので少ない回数(1回くらい)で良いようです。

ついでに $q$ 分布は正規化さえされていれば、本当に何でもいいので VAE などでは $q\_\phi(z|x)$のように観測変数$x$から潜在変数$z$を推論するエンコーダを使い、デコーダ$p\_\theta(x|z)$と同時にELBOを最大化するように訓練します。このとき $p\_\theta(z)$はただの標準正規分布だったりします。

参考文献

- Diederik P Kingma, Max Welling, "Auto-Encoding Variational Bayes," https://arxiv.org/abs/1312.6114
- Eric Jang, Shixiang Gu, Ben Poole, "Categorical Reparameterization with Gumbel-Softmax," https://arxiv.org/abs/1611.01144
